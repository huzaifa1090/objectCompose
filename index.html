<!DOCTYPE html>
<html>
<meta property='og:title' content='GigaGAN for Text-to-Image Synthesis. CVPR2023'/>
<meta property='og:image' content='https://mingukkang.github.io/GigaGAN/static/images/thumbnail.jpg'/>
<meta property='og:description' content='a 1B parameter large scale GAN for text-to-image synthesis task. CVPR2023'/>
<meta property='og:url' content='https://mingukkang.github.io/GigaGAN/'/>
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website'/>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9VZKE74FPW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-9VZKE74FPW');
</script>
  <meta charset="utf-8">
  <meta name="description"
        content="Scaling up GANs for Text-to-Image Synthesis">
  <meta name="keywords" content="GigaGAN, Text-to-Image, GAN, Image synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GigaGAN: Scaling up GANs for Text-to-Image Synthesis</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">

</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');
</style>


<body>
  <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Scaling up GANs for Text-to-Image Synthesis</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mingukkang.github.io/">Minguk Kang</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://richzhang.github.io/">Richard Zhang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://jaesik.info/">Jaesik Park</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://taesung.me/">Taesung Park</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>POSTECH,</span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>3</sup>Adobe Research</span>
          </div>

          <div class="is-size-5 publication-venue">
            in CVPR 2023 (Highlight)
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="static/paper/gigagan_cvpr2023_compressed1.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (low-res, 16MB)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="static/paper/gigagan_cvpr2023_original1.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (high-res, 49MB)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.05511"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <br>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/ZjxtuDQkOPY"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=UyoXmHS-KGc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Two Minute Papers</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/mingukkang/GigaGAN/tree/main/evaluation"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Evaluation</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://www.cs.cmu.edu/~junyanz/talks/GigaGAN_slides.pptx"
                   class="external-link button is-normal is-rounded is-dark">
                   <svg style="width:16px;height:16px;margin-left:-0px;margin-right:6px" width="16px" height="16px" viewBox="0 0 44 44"><path fill="currentColor" d="M10,26h8a2,2,0,0,0,2-2V14a2,2,0,0,0-2-2H10a2,2,0,0,0-2,2V24A2,2,0,0,0,10,26Zm0-12h8V24H10Zm15,4H35a1,1,0,0,0,0-2H25a1,1,0,0,0,0,2Zm0-4h6a1,1,0,0,0,0-2H25a1,1,0,0,0,0,2Zm0,8h4a1,1,0,0,0,0-2H25a1,1,0,0,0,0,2ZM42,2H24a2,2,0,0,0-4,0H2A2,2,0,0,0,0,4V6A2,2,0,0,0,2,8V32a2,2,0,0,0,2,2H21v2.59l-5.71,5.71a1,1,0,1,0,1.41,1.41L22,38.41l5.29,5.29a1,1,0,1,0,1.41-1.41L23,36.59V34H40a2,2,0,0,0,2-2V8a2,2,0,0,0,2-2V4A2,2,0,0,0,42,2ZM40,32H4V8H40ZM42,6H2V4H42ZM25,26H35a1,1,0,0,0,0-2H25a1,1,0,0,0,0,2Z"></path>
                   </svg>
                  <span>Slides</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">GigaGAN: Large-scale GAN for Text-to-Image Synthesis</h2>
      <div class="content has-text-justified">
        <p>
          <!--<b>A large 1B-parameter GAN model for text-to-image, with a competitive FID, disentangled latent space, and a fast upsampler for generating 4K outputs.</b>-->
          <b>Can GANs also be trained on a large dataset for a general text-to-image synthesis task?</b> We present our 1B-parameter GigaGAN, achieving lower FID than Stable Diffusion v1.5, DALLÂ·E 2, and Parti-750M. It generates 512px outputs at 0.13s, orders of magnitude faster than diffusion and autoregressive models, and inherits the disentangled, continuous, and controllable latent space of GANs. We also train a fast upsampler that can generate 4K images from the low-res outputs of text-to-image models.
          <br> 
        </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-t2i0">
          <img id="myt2i0" src="./static/images/Text2img.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-t2i1">
          <img id="myt2i1" src="./static/images/Text2img2.jpg"
          class="interpolation-image"/>
        </div>
        <div class="item item-t2i1">
          <img id="myt2i1" src="./static/images/Text2img3.jpg"
          class="interpolation-image"/>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Disentangled Prompt Interpolation</h2>
    <div class="has-text-justified">
      <p>
        GigaGAN comes with a disentangled, continuous, and controllable latent space.
        <br>In particular, it can achieve layout-preserving fine style control by applying a different prompt at fine scales. 
      </p>
    </div>
    <br>
    
    <div class="columns is-centered">

      
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/teddybear_interpolation.mp4"
                    type="video/mp4">
          </video>
          <div class="has-text-justified">
          <p>
            <b>Changing texture with prompting.</b>
            At coarse layers, we use the prompt <span style="font-family:menlo;size:0.8;">"A teddy bear on tabletop"</span> to fix the layout. Then at fine layers, we use <span style="font-family:menlo;size:0.8;">"A teddy bear with the texture of [fleece, crochet, denim, fur] on tabletop"</span>. <a href="https://youtube.com/shorts/KpaVP6cduhk?feature=share" style="color: blue">(Youtube link)</a>
          </p>
          </div>
          
          
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/house_interpolation.mp4"
                      type="video/mp4">
            </video>
            <div class="has-text-justified">
            <p>
              <b>Changing style with prompting.</b>
              At coarse layers, we use the prompt <span style="font-family:menlo;size:0.8;">"A mansion"</span> to fix the layout. Then at fine layers, we use <span style="font-family:menlo;size:0.8;">"A [modern, Victorian] mansion in [sunny day, dramatic sunset]"</span>. <a href="https://youtube.com/shorts/_eDwU-GQcKo" style="color: blue">(Youtube link)</a>
            </p>
            </div>
            
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Upscaling to 16-megapixel photos with GigaGAN</h2>
      <div class="has-text-justified">
      <p>
        Our GigaGAN framework can also be used to train an efficient, higher-quality upsampler. This can be applied to real images, or to the outputs of other text-to-image models like diffusion. GigaGAN can synthesize ultra high-res images at 4k resolution in 3.66 seconds.
      </p>
      </div>
      <!-- The expanding image container -->
      <div class="tab_container">
        <!-- Close the image -->
        <!-- <span onclick="this.parentElement.style.display='none'" class="closebtn">&times;</span> -->

        <!-- Expanded image -->
        <div id="juxtapose-embed" data-startingposition="30%" data-animate="true">
        </div>

        <div>
          <div id="juxtapose-hidden"></div>
        </div>
        
        <!-- Image text -->
        <div id="imgtext"></div>
      </div>

      <!-- The grid: four columns -->
      <div class="tab_row">
        <div class="tab_column">
          <img src="./static/images/iguana_input.jpg" onclick="tab_gallery_click('iguana');">

        </div>
        <div class="tab_column">
          <img src="./static/images/dog_input.jpg"  onclick="tab_gallery_click('dog');">
        </div>
        <div class="tab_column">
          <img src="./static/images/turtle_input.jpg" onclick="tab_gallery_click('turtle');">
        </div>
        <div class="tab_column">
          <img src="./static/images/kitten_input.jpg"  onclick="tab_gallery_click('kitten');">
        </div>
        <div class="tab_column">
          <img src="./static/images/guy_input.jpg"  onclick="tab_gallery_click('guy');">
        </div>
        <div class="tab_column">
          <img src="./static/images/boy_input.jpg"  onclick="tab_gallery_click('boy');">
        </div>
        <div class="tab_column">
          <img src="./static/images/horse_input.jpg"  onclick="tab_gallery_click('horse');">
        </div>
        <div class="tab_column">
          <img src="./static/images/pancake_input.jpg"  onclick="tab_gallery_click('pancake');">
        </div>
        <div class="tab_column">
          <img src="./static/images/elephant_input.jpg"  onclick="tab_gallery_click('elephant');">
        </div>
        <div class="tab_column">
          <img src="./static/images/plant_input.jpg"  onclick="tab_gallery_click('plant');">
        </div>
      </div>

      
      
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination.
            From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models.
            GANs used to be the de facto choice, with techniques like StyleGAN. With DALL&#183;E 2, auto-regressive and diffusion models became the new standard for large-scale generative models overnight.
            This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? 
            We find that na&#207;vely increasing the capacity of the StyleGAN architecture quickly becomes unstable.
            We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis.
            GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image.
            Second, it can synthesize high-resolution images, for example, 16-megapixel pixels in 3.66 seconds.
            Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">GigaGAN architecture</h2>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">GigaGAN generator</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/G_architecture.jpg">
        </div>
        <div class="content has-text-justified">
          <p>
            GigaGAN generator consists of a <b>text encoding branch</b>, <b>style mapping network</b>, <b>multi-scale synthesis network</b>, augmented by <b>stable attention</b> and <b>adaptive kernel selection</b>. 
            In the <b>text encoding branch</b>, we first extract text embeddings using a pretrained CLIP model and a learned attention layers <b>T</b>. The embedding is passed to the <b>style mapping network M</b> to produce the style vector <b>w</b>, similar to StyleGAN. Now the <b>synthesis network</b> uses the style code as modulation and the text embeddings as attention to produce an image pyramid. Furthermore, we introduce <b>sample-adaptive kernel selection</b> to adaptively choose convolution kernels based on the input text conditioning.
          </p>
        </div>
        <!-- Prompt Interpolation image -->

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">GigaGAN discriminator</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/D_architecture.jpg" width="50%"">
        </div>
        <div class="content has-text-justified">
          <p>
            Similar to the generator, our discriminator consists of two branches for processing the image and the text conditioning. The <b>text branch</b> processes the text similar to the generator. 
            The <b>image branch</b> receives an image pyramid and makes <b>independent</b> predictions for each image scale. Moreover, the predictions are made at all subsequent scales of the downsampling layers. We also employ additional losses to encourage effective convergence. Please see our paper for full details.
          </p>
        </div>
        <!-- Prompt Interpolation image -->

      </div>
    </div>

    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Latent space editing applications</h2>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4 has-text-justified">Prompt interpolation</h3>
        <div class="content has-text-justified">
          <p>
            GigaGAN enables smooth interpolation between prompts, as shown in the interpolation grid.
            The four corners are generated from the same latent z but with different text prompts.
          </p>
        </div>
          <!-- Prompt Interpolation image -->
          <div class="container is-max-desktop">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item item-sunflowers">
                <img src="./static/images/sunflowers.png"
                class="interpolation-image"/>
              </div>
              <div class="item item-victoria">
                <img src="./static/images/victorian_mansion.png"
                class="interpolation-image"/>
              </div>
              <div class="item item-house">
                <img src="./static/images/interpolation_house.png"
                class="interpolation-image"/>
              </div>
            </div>
          </div>
        <!-- Prompt Interpolation image -->
        
        <br>
        <h3 class="title is-4 has-text-justified">Disentangled prompt mixing</h3>
        <div class="content has-text-justified">
          <p>
            GigaGAN retains a disentangled latent space, enabling us to combine the coarse style of one sample with the fine style of another.
            Moreover, GigaGAN can directly control the style with text prompts.
          </p>
        </div>
        <div class="content has-text-centered">
            <img src="./static/images/prompt_mixing.png"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
        </div>
        <!-- Prompt mixing -->

        <!-- Style swapping -->
        <h3 class="title is-4 has-text-justified">Coarse-to-fine sytle swapping</h3>
        <div class="content has-text-justified">
          <p>
            Our GAN-based architecture retains a disentangled latent space, enabling us to blend the coarse style of one sample with the fine style of another.
          </p>
        </div>
        <!-- Prompt Interpolation image -->
        <div class="container is-max-desktop">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-male">
              <img src="./static/images/style_swapping.jpg"
              class="interpolation-image"/>
            </div>
            <div class="item item-car">
              <img src="./static/images/style_swapping_male.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-house">
              <img src="./static/images/style_swapping_house.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>

    <!--/ Animation. -->
    <br>
    <h3 class="title is-3">Click images to see more examples</h3>
    <div id="player" style="cursor:pointer;">
    </div>

    <script>
      class ClickListener {
        constructor(images) {
          this.rootElement = document.querySelector( "#player" );
          this.element = document.createElement("img");
          this.rootElement.appendChild(this.element);
          this.images = images
          this.idx = 0
          this.k = 0;
          this.init()
        }

        init() {
          this.element.src = this.images[this.idx]
          this.element.style.width = `320px`;
          this.element.style.height = `320px`;
          this.element.onclick = this.nextImage.bind(this); 
        }

        nextImage() {
          this.idx++;
          if(this.idx >= this.images.length) {
            this.idx = 0;
          }
            setTimeout(this.identity(), 300); setTimeout(this.swapping(), 300);
          }
  

        swapping() {
          this.k += 90;
          this.element.style.transitionDuration = "0.5s";
          this.element.style.transform = "rotatey(" + this.k + "deg)";
          this.element.src = this.images[this.idx];
        }

        identity() {
          this.k += 90;
          this.element.style.transitionDuration = "0.5s";
          this.element.style.transform = "rotatey(" + this.k + "deg)";
          this.element.src = this.element.src;
        }
        }

      class ClickListener1 {
      constructor(images) {
        this.rootElement = document.querySelector( "#player" );
        this.element = document.createElement("img");
        this.rootElement.appendChild(this.element);
        this.images = images
        this.idx = 0
        this.init()
      }
    
      init() {
        this.element.src = this.images[this.idx]
        this.element.style.width = `320px`;
        this.element.style.height = `50px`;
        this.element.onclick = this.nextImage.bind(this)
      }
    
      nextImage() {
        this.idx++;
        if(this.idx >= this.images.length) {
          this.idx = 0;
        }
        this.element.src = this.images[this.idx]
        }
      }
    </script>

    <script>
    new ClickListener(['./static/images/car0.jpg', './static/images/car1.jpg', './static/images/car2.jpg', './static/images/car3.jpg', './static/images/car4.jpg'])
    new ClickListener(['./static/images/galaxy0.jpg', './static/images/galaxy1.jpg', './static/images/galaxy2.jpg', './static/images/galaxy3.jpg', './static/images/galaxy4.jpg'])
    new ClickListener(['./static/images/newton0.jpg', './static/images/newton1.jpg', './static/images/newton2.jpg', './static/images/newton3.jpg', './static/images/newton4.jpg',])
    new ClickListener1(['./static/images/caption0_0.png',])
    new ClickListener1(['./static/images/caption0_1.png',])
    new ClickListener1(['./static/images/caption0_2.png',])
    new ClickListener(['./static/images/sea0.jpg', './static/images/sea1.jpg', './static/images/sea2.jpg', './static/images/sea3.jpg', './static/images/sea4.jpg'])
    new ClickListener(['./static/images/candle2.jpg', './static/images/candle1.jpg', './static/images/candle3.jpg', './static/images/candle0.jpg', './static/images/candle4.jpg'])
    new ClickListener(['./static/images/bread0.jpg', './static/images/bread1.jpg', './static/images/bread2.jpg', './static/images/bread3.jpg', './static/images/bread4.jpg',])
    new ClickListener1(['./static/images/caption1_0.png',])
    new ClickListener1(['./static/images/caption1_1.png',])
    new ClickListener1(['./static/images/caption1_2.png',])
    new ClickListener(['./static/images/alps0.jpg', './static/images/alps1.jpg', './static/images/alps2.jpg', './static/images/alps3.jpg', './static/images/alps4.jpg'])
    new ClickListener(['./static/images/light0.jpg', './static/images/light1.jpg', './static/images/light2.jpg', './static/images/light3.jpg', './static/images/light4.jpg'])
    new ClickListener(['./static/images/castle0.jpg', './static/images/castle1.jpg', './static/images/castle2.jpg', './static/images/castle3.jpg', './static/images/castle4.jpg'])
    new ClickListener1(['./static/images/caption2_0.png',])
    new ClickListener1(['./static/images/caption2_1.png',])
    new ClickListener1(['./static/images/caption2_2.png',])
    </script>
    
    <br>

    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
      <!-- Latent space editing applications -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Related works</h2>
          <div class="content has-text-justified">
            <p>
              <li>Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. <a href="https://arxiv.org/abs/2112.09130">Ensembling Off-the-shelf Models for GAN Training.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li><br>
              <li>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. <a href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis with latent diffusion models.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li><br>
              <li>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. <a href="https://arxiv.org/abs/2103.00020">Learning transferable visual models from natural language supervision.</a> In International Conference on Machine Learning (ICML), 2021.</li><br>
              <li>Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. <a href="https://arxiv.org/abs/1912.04958">Analyzing and Improving the Image Quality of StyleGAN.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</li><br>
              <li>Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. <a href="https://arxiv.org/abs/1612.03242">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.</a> In IEEE International Conference on Computer Vision (ICCV), 2017.</li><br>
              <li> Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. <a href="https://arxiv.org/abs/1605.05396">Generative Adversarial Text to Image Synthesis</a>. In International Conference on Machine Learning (ICML), 2016.</li><br>
            </p>
          </div>
          <!-- Prompt Interpolation image -->
        </div>
      </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
            We thank Simon Niklaus, Alexandru Chiculita, and Markus Woodson for building the distributed training pipeline. We thank Nupur Kumari, Gaurav Parmar, Bill Peebles, Phillip Isola, Alyosha Efros, and Joonghyuk Shin for their helpful comments.
            We also want to thank Chenlin Meng, Chitwan Saharia, and Jiahui Yu for answering many questions about their fantastic work. We thank Kevin Duarte for discussions regarding upsampling beyond 4K.
            Part of this work was done while Minguk Kang was an intern at Adobe Research.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@inproceedings{kang2023gigagan,
  author    = {Kang, Minguk and Zhu, Jun-Yan and Zhang, Richard and Park, Jaesik and Shechtman, Eli and Paris, Sylvain and Park, Taesung},
  title     = {Scaling up GANs for Text-to-Image Synthesis},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/iguana_input.jpg", "label": "Input Photos or Artworks (128px)",},
  {"src": "./static/images/iguana_output.jpg", "label": "Upsampled by GigaGAN (4K)",}
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";


function tab_gallery_click(name) {
  // Get the expanded image
  let inputImage = {
    label: "Input Photos or Artworks (128px)",
  };
  let outputImage = {
    label: "Upsampled by GigaGAN (4K)",
  };

  inputImage.src = "./static/images/".concat(name, "_input.jpg")
  outputImage.src = "./static/images/".concat(name, "_output.jpg")

  let images = [inputImage, outputImage];
  let options = slider.options;
  options.callback = function(obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);
      
  };
  
  slider = new juxtapose.JXSlider(transientSelector, images, options);
};



(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
    //document.getElementById("left-button").onclick = replaceLeft;
    //document.getElementById("right-button").onclick = replaceRight;
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
